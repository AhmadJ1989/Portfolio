{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f034b440",
      "metadata": {
        "id": "f034b440"
      },
      "source": [
        "\n",
        "# **NLP Workshop Notebook**\n",
        "This workshop covers real-world NLP preprocessing techniques and a hands-on text classification task.\n",
        "\n",
        "## **Learning Objectives**\n",
        "- Tokenize text\n",
        "- Remove stopwords\n",
        "- Perform stemming and lemmatization\n",
        "- Apply POS tagging and Named Entity Recognition\n",
        "- Extract noun chunks (compound terms)\n",
        "- Train a basic sentiment classifier\n",
        "- Evaluate model performance\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aa3fcdc",
      "metadata": {
        "id": "7aa3fcdc"
      },
      "source": [
        "## **1. Setup and Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c1fa1eb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1fa1eb6",
        "outputId": "3e8cf84f-2db5-419b-c6e4-d0bcc4448c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe71aff1",
      "metadata": {
        "id": "fe71aff1"
      },
      "source": [
        "## **2. Sample Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13128a79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13128a79",
        "outputId": "518c4504-ae6f-4bdb-c8cf-f9f2f426a57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is a field of artificial intelligence\n",
            "that gives computers the ability to understand text and spoken words. NLP started by applying rule-based techniques. Currently it uses transformers.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"\"\"Natural language processing (NLP) is a field of artificial intelligence\n",
        "that gives computers the ability to understand text and spoken words. NLP started by applying rule-based techniques. Currently it uses transformers.\"\"\"\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a312111",
      "metadata": {
        "id": "1a312111"
      },
      "source": [
        "## **3. Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "11c80bcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c80bcd",
        "outputId": "f6bd4961-90aa-40ef-c1bb-bf9a12eac3e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'field',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'that',\n",
              " 'gives',\n",
              " 'computers',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'text',\n",
              " 'and',\n",
              " 'spoken',\n",
              " 'words',\n",
              " '.',\n",
              " 'NLP',\n",
              " 'started',\n",
              " 'by',\n",
              " 'applying',\n",
              " 'rule-based',\n",
              " 'techniques',\n",
              " '.',\n",
              " 'Currently',\n",
              " 'it',\n",
              " 'uses',\n",
              " 'transformers',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "tokens = word_tokenize(text)\n",
        "tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokens\n",
        "Tokens2=sent_tokenize(text)\n",
        "Tokens2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hazbcWCl-AEV",
        "outputId": "f718ed2a-87a1-4fdb-bffd-9704838bf4ab"
      },
      "id": "hazbcWCl-AEV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing (NLP) is a field of artificial intelligence\\nthat gives computers the ability to understand text and spoken words.',\n",
              " 'NLP started by applying rule-based techniques.',\n",
              " 'Currently it uses transformers.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76bdfa8e",
      "metadata": {
        "id": "76bdfa8e"
      },
      "source": [
        "**Exercise 1:** Tokenize your own sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a278e485",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a278e485",
        "outputId": "e6951c39-097f-489e-e744-45de34f366fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Rasha',\n",
              " '.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'Egyptian',\n",
              " '.',\n",
              " 'I',\n",
              " 'have',\n",
              " '3',\n",
              " 'children',\n",
              " '.',\n",
              " 'I',\n",
              " 'live',\n",
              " 'in',\n",
              " 'Cairo',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "your_text = \"My name is Rasha. I'm Egyptian. I have 3 children. I live in Cairo.\"\n",
        "word_tokenize(your_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce1ce5d",
      "metadata": {
        "id": "5ce1ce5d"
      },
      "source": [
        "## **4. Stopword Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5de4fa89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5de4fa89",
        "outputId": "8f1e4bf3-d89a-4831-b42a-71ab339cef0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'had', \"he'll\", 'themselves', 'than', 'ain', 'don', 'but', 'shan', 'who', 'both', 'can', \"couldn't\", 'just', 'against', \"i'll\", 'aren', \"didn't\", \"it'd\", 'from', 'more', 'and', 'until', 'me', \"that'll\", 'the', 'as', 'does', \"they'll\", 'an', 'having', 'too', 'off', 'wouldn', 'it', 'with', \"you've\", 'do', 'so', \"she'd\", 'couldn', 'own', 'that', 'if', \"should've\", 'here', 'needn', 'she', 'then', 'should', 'further', 'have', \"needn't\", 'were', 'about', 'why', \"haven't\", \"they're\", \"you're\", \"i'm\", \"weren't\", 'between', 'ours', 'during', \"he'd\", 'did', 'wasn', 'mightn', 'hasn', 'by', 'into', 'her', 'out', 'once', 'll', 'hadn', 'our', 'because', 'was', 'isn', \"you'll\", 's', 'up', \"hasn't\", \"we've\", 'y', 'been', \"hadn't\", 'mustn', 'these', \"isn't\", 'd', 'they', 'after', \"doesn't\", 'such', 'where', 'nor', 'doing', 'has', 'being', 'only', 'a', 'is', 'under', 'yours', 'shouldn', \"won't\", 're', \"shouldn't\", 'of', 'his', 'your', \"we'd\", 'those', \"shan't\", 'my', \"aren't\", \"don't\", 'haven', 'all', 'other', 'now', \"mustn't\", 'ma', 'won', 'o', 'yourself', \"wouldn't\", 'him', 'very', 've', 'in', 'any', 'while', 'm', 'to', 'down', \"she'll\", \"it'll\", \"we'll\", 'be', 'theirs', 'over', 'himself', \"they'd\", 'this', \"wasn't\", 'which', 'each', 'through', 'for', 'not', 'i', \"it's\", 'herself', 'he', 'yourselves', \"we're\", \"i'd\", 'their', 'we', \"she's\", 'when', \"he's\", 'its', 'whom', 'are', \"i've\", 'what', 'or', 'didn', 'above', 'some', 'weren', 'again', 'there', 'you', \"you'd\", 'no', 'am', \"mightn't\", 'hers', 'below', 'at', 'ourselves', 'them', 'will', 'itself', 'most', 'before', 't', 'on', 'doesn', 'myself', 'few', 'how', \"they've\", 'same'}\n"
          ]
        }
      ],
      "source": [
        "# check stop words in the English corpus\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of text words after removinf the stop words\n",
        "filtered = [w for w in tokens if w.lower() not in stop_words]  #list_comprehension\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNPCi9icEU3_",
        "outputId": "6215b953-8013-4aaa-e3f0-37cabda8fbd6"
      },
      "id": "aNPCi9icEU3_",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', 'gives', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', '.', 'NLP', 'started', 'applying', 'rule-based', 'techniques', '.', 'Currently', 'uses', 'transformers', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punct marks\n",
        "filtered = [w for w in filtered if w.isalpha()]  #if not letter (alpha) then remove it - it keeps only letters\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFouKIIuE9VH",
        "outputId": "0d07ee38-fd35-439d-a31a-742dd7196123"
      },
      "id": "pFouKIIuE9VH",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'NLP', 'field', 'artificial', 'intelligence', 'gives', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'NLP', 'started', 'applying', 'techniques', 'Currently', 'uses', 'transformers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39362a64",
      "metadata": {
        "id": "39362a64"
      },
      "source": [
        "## **5. Stemming and Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "83a0cd14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83a0cd14",
        "outputId": "67307d21-a7f3-43ff-abac-827d0e4a6795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'give', 'comput', 'abil', 'understand', 'text', 'spoken', 'word', 'nlp', 'start', 'appli', 'techniqu', 'current', 'use', 'transform']\n",
            "['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'give', 'comput', 'abil', 'understand', 'text', 'spoken', 'word', 'nlp', 'start', 'appli', 'techniqu', 'current', 'use', 'transform']\n"
          ]
        }
      ],
      "source": [
        "#stemming\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "stemmed = [ps.stem(w) for w in filtered]\n",
        "\n",
        "print(stemmed )\n",
        "\n",
        "\n",
        "snow=SnowballStemmer(language='english')\n",
        "stemmed2=[snow.stem(w) for w in filtered]\n",
        "print(stemmed2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmitization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
        "lemmatized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JntD309NFx1Y",
        "outputId": "35a2fe53-2b93-42b9-cf4e-a37b2c016d7d"
      },
      "id": "JntD309NFx1Y",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'NLP',\n",
              " 'field',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'give',\n",
              " 'computer',\n",
              " 'ability',\n",
              " 'understand',\n",
              " 'text',\n",
              " 'spoken',\n",
              " 'word',\n",
              " 'NLP',\n",
              " 'started',\n",
              " 'applying',\n",
              " 'technique',\n",
              " 'Currently',\n",
              " 'us',\n",
              " 'transformer']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming - lemmatization\n",
        "print(ps.stem('older'))\n",
        "print(snow.stem('older'))\n",
        "print(lemmatizer.lemmatize('older', pos='a'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699S6gTWHglz",
        "outputId": "b41bb696-9c73-44e8-c1ce-05b7e56e4d35"
      },
      "id": "699S6gTWHglz",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "older\n",
            "older\n",
            "old\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6668c622",
      "metadata": {
        "id": "6668c622"
      },
      "source": [
        "## **6. POS Tagging and NER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "512c3d6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "512c3d6b",
        "outputId": "53d209e2-b3ab-48f4-b8f7-a08d2c522114"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('Andrew', 'PROPN'),\n",
              "  ('Yan', 'PROPN'),\n",
              "  ('-', 'PUNCT'),\n",
              "  ('Tak', 'PROPN'),\n",
              "  ('Ng', 'PROPN'),\n",
              "  ('is', 'AUX'),\n",
              "  ('a', 'DET'),\n",
              "  ('British', 'ADJ'),\n",
              "  ('-', 'PUNCT'),\n",
              "  ('American', 'ADJ'),\n",
              "  ('computer', 'NOUN'),\n",
              "  ('scientist', 'NOUN'),\n",
              "  ('and', 'CCONJ'),\n",
              "  ('technology', 'NOUN'),\n",
              "  ('entrepreneur', 'NOUN'),\n",
              "  ('focusing', 'VERB'),\n",
              "  ('on', 'ADP'),\n",
              "  ('machine', 'NOUN'),\n",
              "  ('learning', 'NOUN'),\n",
              "  ('and', 'CCONJ'),\n",
              "  ('artificial', 'ADJ'),\n",
              "  ('intelligence', 'NOUN'),\n",
              "  ('.', 'PUNCT'),\n",
              "  ('He', 'PRON'),\n",
              "  ('worked', 'VERB'),\n",
              "  ('at', 'ADP'),\n",
              "  ('Google', 'PROPN'),\n",
              "  ('for', 'ADP'),\n",
              "  ('13', 'NUM'),\n",
              "  ('years', 'NOUN'),\n",
              "  ('.', 'PUNCT'),\n",
              "  ('He', 'PRON'),\n",
              "  ('established', 'VERB'),\n",
              "  ('DeepLearning', 'PROPN'),\n",
              "  ('platform', 'NOUN'),\n",
              "  ('in', 'ADP'),\n",
              "  ('2002', 'NUM'),\n",
              "  ('.', 'PUNCT')],\n",
              " [('Andrew Yan-Tak Ng', 'PERSON'),\n",
              "  ('British', 'NORP'),\n",
              "  ('Google', 'ORG'),\n",
              "  ('13 years', 'DATE'),\n",
              "  ('DeepLearning', 'ORG'),\n",
              "  ('2002', 'DATE')])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "\n",
        "text3='Andrew Yan-Tak Ng is a British-American computer scientist and technology entrepreneur focusing on machine learning and artificial intelligence. He worked at Google for 13 years. He established DeepLearning platform in 2002.'\n",
        "doc = nlp(text3)\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "pos, entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d3d77e",
      "metadata": {
        "id": "17d3d77e"
      },
      "source": [
        "## **7. Compound Term Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b777eeb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b777eeb1",
        "outputId": "7582ea06-bba6-45fc-dec3-1b2cad872361"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Andrew Yan-Tak Ng,\n",
              " a British-American computer scientist and technology entrepreneur,\n",
              " machine learning,\n",
              " artificial intelligence,\n",
              " He,\n",
              " Google,\n",
              " 13 years,\n",
              " He,\n",
              " DeepLearning platform]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "\n",
        "list(doc.noun_chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63407ad5",
      "metadata": {
        "id": "63407ad5"
      },
      "source": [
        "---\n",
        "# **8. Text Classification Task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdde66bd",
      "metadata": {
        "id": "cdde66bd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/mnt/data/sentiment_dataset.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ee4f8a",
      "metadata": {
        "id": "11ee4f8a"
      },
      "outputs": [],
      "source": [
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6978c6d2",
      "metadata": {
        "id": "6978c6d2"
      },
      "source": [
        "## **Exercise 2:**\n",
        "Try replacing the classifier with `LogisticRegression`. Compare results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75388394",
      "metadata": {
        "id": "75388394"
      },
      "source": [
        "---\n",
        "# **Answer Key**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404b1d20",
      "metadata": {
        "id": "404b1d20"
      },
      "source": [
        "\n",
        "### ✅ Answer for Exercise 1:\n",
        "Use `word_tokenize(your_text)` — the output should be a list of tokens.\n",
        "\n",
        "### ✅ Answer for Exercise 2:\n",
        "Replace the model section with:\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}